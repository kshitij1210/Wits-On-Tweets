{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Author:WitsOnTweets\n",
    "    #This module helps to solve the cases of negation of sentiment of sentences by not, n't, no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Usage:Libraries\n",
    "    #numpy:python library for array manipulation\n",
    "    #nltk:Natural Language Toolkit for Basic Text Processing . Here it is used to remove Stopwords(Words that dont convey any meaning)\n",
    "    #pandas:pandas library providing high-performance, easy-to-use data structures and data analysis\n",
    "    #WordNetLemmatizer:Converts the word to their Base Form\n",
    "    #string:for removing punctuations in input string\n",
    "    #re:for using regular expressions to remove emoticons and urls\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets after stopword and punctuation marks removal : \n",
      "1 .) lol luv gt gt any1 find c 1 \n",
      "2 .) amp actually notbad enough cant anything loll poor pitbull girl \n",
      "3 .) neverthought word quot randomly quot could funny husband said cracking haha goodnight blessed \n",
      "4 .) want watch super junior video catherine sent scared looks mary \n"
     ]
    }
   ],
   "source": [
    "#converting string to tokens and lemmatize(convert to base form) it\n",
    "#IDEA:\n",
    "#Steps:\n",
    "#-->Break Sentence in tokens\n",
    "#-->lemmatize each token\n",
    "#-->if token is n't, not, no then append in next word\n",
    "#-->so not bad becomes not_bad and treated as a new word\n",
    "#-->concat the words to form the new sentence \n",
    "#-->update the dataset\n",
    "ii=1\n",
    "stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "xyz=open('2lakh1.csv','w')\n",
    "training_data = np.genfromtxt('testing.csv', delimiter=',', skip_header=1, usecols=(1, 3), dtype=None)\n",
    "for sentence in training_data:\n",
    "    label=str(sentence[0])\n",
    "    line=str(sentence[1])\n",
    "    line=line+\" .\"\n",
    "    line=line[2:-1].lower()#dataset has \"b in starting \" in end .. ignoring that \n",
    "    line = ' '.join(re.sub(r\"(@[^ ]+)|(#[^ ]+)\",\" \",line).split())\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    line = emoji_pattern.sub(r'',line)\n",
    "    line = re.sub(r\"http\\S+\", \"\", line)\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    line=line+\" .\"\n",
    "    words = word_tokenize(line)\n",
    "    lem_words=[lemmatizer.lemmatize(x) for x in words]\n",
    "    new_sentence=\"\"\n",
    "    fixed_sentence=\"\"\n",
    "    count=0\n",
    "    flag=0\n",
    "    new_words=[]\n",
    "    for i in lem_words:\n",
    "        flag=flag+1\n",
    "        count=count+1\n",
    "        if i == \"not\" or i == \"no\" or i == \"n't\" or i == \"never\":\n",
    "            i=i+\"_\"+lem_words[count]\n",
    "            flag=-1\n",
    "        if flag !=0:\n",
    "            new_words.append(i)\n",
    "    for i in new_words:\n",
    "        new_sentence=new_sentence+i+\" \"\n",
    "    new_sentence = ''.join(e for e in new_sentence if e.isalnum() or e == ' ')\n",
    "    for i in word_tokenize(new_sentence.lower()):\n",
    "        if i not in stop:\n",
    "            fixed_sentence=fixed_sentence+i+\" \"\n",
    "    new_str =''.join([char for char in fixed_sentence if char not in punctuation])\n",
    "    new_str.encode(\"utf-8\")\n",
    "    xyz.write(\"%s,%s\\n\" %( label,(new_str)))\n",
    "xyz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope u have cheeSe burgerS 4 LMBO\n"
     ]
    }
   ],
   "source": [
    "fixed_sentence = \" @Hijack_King7 I hope u have cheeSe burgerS 4 @Snubbmatic LMBO\"\n",
    "fixed_sentence = ' '.join(re.sub(r\"(@[^ ]+)|(#[^ ]+)\",\" \",fixed_sentence).split())\n",
    "print(fixed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
