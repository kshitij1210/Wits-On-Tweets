{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Author:WitsOnTweets\n",
    "    #This module helps to train and make the model which is used to predict the sentiment of a sentence.\n",
    "#Dataset:Sentiment140 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Usage:Libraries\n",
    "    #keras:keras is high level Neural Network API written in python. It works on top of Theano, CNTK, Tensorflow. Here we chose \n",
    "    #Tensorflow as the backend for processing\n",
    "    #Tokenizer:Helps to break the sentence in Tokens(Words)\n",
    "    #numpy:python library for array manipulation\n",
    "    #nltk:Natural Language Toolkit for Basic Text Processing . Here it is used to remove Stopwords(Words that dont convey any meaning)\n",
    "    #pandas:pandas library providing high-performance, easy-to-use data structures and data analysis\n",
    "    #EarlyStopping:Stops the Training Process after the validation accuracy stops increasing thus preventing overfitting of our model\n",
    "    \n",
    "\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iloc[:,1] reads all the rows and picks the first column in it\n",
    "\n",
    "\n",
    "training = pd.read_csv('2lakh1.csv', usecols=(0, 1),encoding=\"latin-1\")\n",
    "stop = set(stopwords.words('english'))\n",
    "train_x=training.iloc[:,1].astype(str)\n",
    "train_y=training.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 3000\n",
    "#Create a tokenizer using Keras and input the tweets\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "#The dictionary is a map of different words from the tweets and the corresponding index\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "#Saving the dictionary to a json file so we can use it for processing Tweets while testing the model\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "#text_to_word_sequence converts the all the tweets to same length i.e pads all the short tweets\n",
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allWordIndices = []\n",
    "\n",
    "#Using the dictionary convert all the text in the tweets to numbers and feed them to neural net and skip the unindexed words\n",
    "\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "#convert all tweets to an array of lists\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "#create a 3000 length one hot vector for each tweet where every word is 0 or 1 \n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "\n",
    "\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 177592 samples, validate on 19733 samples\n",
      "Epoch 1/6\n",
      "177592/177592 [==============================] - 270s 2ms/step - loss: 0.5311 - acc: 0.7330 - val_loss: 0.4784 - val_acc: 0.7688\n",
      "Epoch 2/6\n",
      "177592/177592 [==============================] - 272s 2ms/step - loss: 0.4859 - acc: 0.7628 - val_loss: 0.4705 - val_acc: 0.7761\n",
      "Epoch 3/6\n",
      "177592/177592 [==============================] - 275s 2ms/step - loss: 0.4495 - acc: 0.7824 - val_loss: 0.4720 - val_acc: 0.7774\n",
      "Epoch 4/6\n",
      "177592/177592 [==============================] - 270s 2ms/step - loss: 0.4113 - acc: 0.8028 - val_loss: 0.4905 - val_acc: 0.7757\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "#NOW THE MODEL BEGINS\n",
    "#Type of model: Sequential Model\n",
    "#HyperParameters:\n",
    "    #No of layers: 4\n",
    "    #Input layer nodes:3000\n",
    "    #Hidden Layer1: 512\n",
    "    #Hidden Layer2: 256\n",
    "    #Output Layer: 2\n",
    "    #DropOut: 0.5\n",
    "    #BatchSize: 28\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "model = Sequential(model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "early_stopping=EarlyStopping(monitor='val_acc',mode='max')\n",
    "\n",
    "#Shuffling the dataset and start Training\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size=28,\n",
    "    epochs=6,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,callbacks=[early_stopping])\n",
    "\n",
    "#Saving the model and weights associated with each edge\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
